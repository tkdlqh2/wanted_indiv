{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugGR7pnI4WSe"
      },
      "source": [
        "# Week3_1 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- 토크나이징이 완료된 위키 백과 코퍼스를 다운받고 **단어 사전을 구축하는 함수를 구현**할 수 있다.\n",
        "- `Skip-Gram` 방식의 학습 데이터 셋을 생성하는 **Dataset과 Dataloader 클래스를 구현**할 수 있다.\n",
        "- **Negative Sampling** 함수를 구현할 수 있다. \n",
        "\n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- Skip-Gram을 학습 과정 튜토리얼을 따라하며, **Skip-Gram을 학습하는 클래스를 구현**할 수 있다. \n",
        "\n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- Skip-Gram 방식으로 word embedding을 학습하는 **Word2Vec 클래스를 구현**하고 실제로 학습할 수 있다.\n",
        "- 학습이 완료된 word embedding을 불러와 **Gensim 패키지를 사용해 유사한 단어**를 뽑을 수 있다. \n",
        "\n",
        "### Reference\n",
        "- [Skip-Gram negative sampling 한국어 튜토리얼](https://wikidocs.net/69141)\n",
        "    - (참고) 위 튜토리얼에서는 target word와 context word 페어의 레이블은 1로, target word와 negative sample word 페어의 레이블은 0이 되도록 학습 데이터를 구현해 binary classification을 구현한다. 하지만 우리는 word2vec 논문 방식을 그대로 따르기 위해 label을 생성하지 않고 대신 loss 함수를 변행해서 binary classification을 학습할 것이다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:29:36.641276Z",
          "start_time": "2022-02-19T14:29:36.638642Z"
        },
        "id": "HlEy3xfY4WSh"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:50:41.644583Z",
          "start_time": "2022-02-19T12:50:41.642937Z"
        },
        "id": "cBrr7-gt4jnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66cf6c91-46a6-44ec-d69b-11a5c7dbd50f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 8.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:26:59.276355Z",
          "start_time": "2022-02-19T14:26:58.411434Z"
        },
        "id": "6mC9lhsJ4WSh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:05.586472Z",
          "start_time": "2022-02-19T14:30:05.583611Z"
        },
        "id": "17g7UZ5g4WSi"
      },
      "outputs": [],
      "source": [
        "# seed\n",
        "seed = 7777\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:06.721039Z",
          "start_time": "2022-02-19T14:30:06.717559Z"
        },
        "id": "v3UlC7Jn4WSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4f391a-feb0-4853-adfb-4fbd46d37a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla T4\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8sfv5KY4WSk"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHs8_LU04WSj"
      },
      "source": [
        "### 토크나이징이 완료된 위키 백과 코퍼스 다운로드 및 불용어 사전 크롤링\n",
        "- 나의 구글 드라이브에 데이터를 다운받아 영구적으로 사용할 수 있도록 하자. \n",
        "    - [데이터 다운로드 출처](https://ratsgo.github.io/embedding/downloaddata.html)\n",
        "- 다운받은 데이터는 토크나이징이 완료된 상태이지만 불용어를 포함하고 있다. 따라서 향후 불용어를 제거하기 위해 불용어 사전을 크롤링하자. \n",
        "    - [불용어 사전 출처](https://www.ranks.nl/stopwords/korean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KYiz1fdNsAqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d300332-085a-4d56-d96d-9b312a40edbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Z2WZ0P4wsAqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef5b74c-5079-48e4-c5bd-73a1fc4ef710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP\n"
          ]
        }
      ],
      "source": [
        "cd  ./drive/MyDrive/NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:11.886643Z",
          "start_time": "2022-02-19T14:27:11.884858Z"
        },
        "id": "4QPBJ6UZ4WSj"
      },
      "outputs": [],
      "source": [
        "# # 데이터 다운로드\n",
        "# !pip install gdown\n",
        "# !gdown https://drive.google.com/u/0/uc?id=1Ybp_DmzNEpsBrUKZ1-NoPDzCMO39f-fx\n",
        "# !unzip tokenized.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.633947Z",
          "start_time": "2022-02-19T14:27:13.829982Z"
        },
        "id": "cTHCHmO24WSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30424539-95db-48e6-a129-49a4877db59b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Korean stop words: 677\n"
          ]
        }
      ],
      "source": [
        "# 한국어 불용어 리스트 크롤링\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.ranks.nl/stopwords/korean\"\n",
        "response = requests.get(url, verify = False)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text,'html.parser')\n",
        "    content = soup.select_one('#article178ebefbfb1b165454ec9f168f545239 > div.panel-body > table > tbody > tr')\n",
        "    stop_words=[]\n",
        "    for x in content.strings:\n",
        "        x=x.strip()\n",
        "        if x:\n",
        "            stop_words.append(x)\n",
        "    print(f\"# Korean stop words: {len(stop_words)}\")\n",
        "else:\n",
        "    print(response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.642775Z",
          "start_time": "2022-02-19T14:27:15.635333Z"
        },
        "id": "3d0IqhDF4WSk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6224ae9a-d583-400e-d2ff-2da56b51f3ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'아'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "stop_words[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t76Q1pQ4WSk"
      },
      "source": [
        "### 단어 사전 구축 함수 구현 \n",
        "- 문서 리스트를 입력 받아 사전을 생성하는 `make_vocab()` 함수를 구현하라.\n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - docs : 문서 리스트\n",
        "        - min_count : 최소 단어 등장 빈도수 (단어 빈도가 `min_count` 미만인 단어는 사전에 포함하지 않음)\n",
        "    - 조건\n",
        "        - 문서 길이 제한\n",
        "            - 단어 개수가 3개 이하인 문서는 처리하지 않음. (skip)\n",
        "        - 사전에 포함되는 단어 빈도수 제한\n",
        "            - 단어가 빈도가 `min_count` 미만은 단어는 사전에 포함하지 않음.\n",
        "        - 불용어 제거 \n",
        "            - 불용어 리스트에 포함된 단어는 제거 \n",
        "    - 반환값 \n",
        "        - word2count : 단어별 빈도 사전 (key: 단어, value: 등장 횟수)\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 (key: 단어 인덱스(int), value: 단어)\n",
        "        - word2wid : 인덱스(wid)별 단어 사전 (key: 단어, value: 단어 인덱스(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:01.637431Z",
          "start_time": "2022-02-19T14:32:56.730711Z"
        },
        "id": "xkjqztIA4WSl"
      },
      "outputs": [],
      "source": [
        "# 코퍼스 로드\n",
        "with open(\"/content/drive/MyDrive/NLP/tokenized/wiki_ko_mecab.txt\",'r') as file:\n",
        "  docs = file.readlines() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:03.423002Z",
          "start_time": "2022-02-19T14:33:03.419818Z"
        },
        "id": "WAKB6bbt4WSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fbd77e2-0e2b-4e47-fcb9-83396d3a42d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# wiki documents: 311,237\n"
          ]
        }
      ],
      "source": [
        "print(f\"# wiki documents: {len(docs):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:04.016885Z",
          "start_time": "2022-02-19T14:33:03.962269Z"
        },
        "id": "-OI1MCXv4WSl"
      },
      "outputs": [],
      "source": [
        "# 문서 개수를 500개로 줄임\n",
        "docs=random.sample(docs,500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"# wiki documents: {len(docs):,}\")"
      ],
      "metadata": {
        "id": "mP5wGu9YwDUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c93c65-a580-4fa1-9d13-cf3ba2e9cb68"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# wiki documents: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:26.392627Z",
          "start_time": "2022-02-19T14:33:26.382358Z"
        },
        "id": "aJaEAVm9sAqv"
      },
      "outputs": [],
      "source": [
        "# 문서 내 숫자, 영어 대소문자, 특수문자를 제거 (re package 사용)\n",
        "import re\n",
        "\n",
        "new_docs = []\n",
        "for doc in docs:\n",
        "  doc = re.sub(r\"[^ ㄱ-ㅣ가-힣+]\",\"\",doc)\n",
        "  new_docs.append(doc)\n",
        "\n",
        "docs = new_docs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Check : {docs[488][:1000]}\")"
      ],
      "metadata": {
        "id": "sytiSICawMk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a4c8c0-f215-4588-c9ce-d1908543563d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check : 히 오 돈목    은 현존 하 는  종 의 어류 로 이루어진 비교 적 원시 적 인 조기어류 목 의 하나 이 다  이 들 은 전통 적 으로 골설어 목 으로 분류 해 왔으며  많 은 학자 들 은 아직 도 그렇 게 분류 하 지만  멸종 한 얀 비 아니 아 속    에 관한 화석 연구 는 히 오 돈목 이 다른 골설어 목 어류 와 초기 에 분리 되 었 음 을 시사 하 고 있 으며  이 때문 에 최근 에 는 별도 의 목 으로 분류 하 고 있 다  생명 의 나무      프로젝트 의 히 오 돈목    정보\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:27.904880Z",
          "start_time": "2022-02-19T14:33:27.899620Z"
        },
        "id": "OAkkQsvO4WSl"
      },
      "outputs": [],
      "source": [
        "def make_vocab(docs:List[str], min_count:int):\n",
        "    \"\"\"\n",
        "    'docs'문서 리스트를 입력 받아 단어 사전을 생성.\n",
        "    \n",
        "    return \n",
        "        - word2count : 단어별 빈도 사전\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 \n",
        "        - word2wid : 인덱스(wid)별 단어 사전\n",
        "    \"\"\"\n",
        "\n",
        "    word2count = dict()\n",
        "    word2id = dict()\n",
        "    id2word = dict()\n",
        "\n",
        "    _word2count={}\n",
        "    for doc in tqdm(docs):\n",
        "        word_list = doc.split()\n",
        "        # 1. 문서 길이 제한 -> 단어개수 3개 이하\n",
        "        # 2. 임시 딕셔너리(_word2count)에 단어별 등장 빈도 기록\n",
        "        # 3. 불용어 제거\n",
        "        if len(word_list)<4 : continue\n",
        "\n",
        "        for word in word_list:\n",
        "          if word not in _word2count:_word2count[word] = 1\n",
        "          else: _word2count[word] +=1\n",
        "    \n",
        "    for word in _word2count:\n",
        "      if word in stop_words: continue\n",
        "    # 4. 토큰 최소 빈도를 만족하는 토큰만 사전에 추가      \n",
        "      if _word2count[word] < min_count:continue\n",
        "      else: word2count[word] = _word2count[word]\n",
        "\n",
        "    id=0\n",
        "    for word in word2count:\n",
        "      word2id[word] = id\n",
        "      id2word[id] = word\n",
        "      id+=1\n",
        "\n",
        "    return word2count, word2id, id2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.157872Z",
          "start_time": "2022-02-19T14:33:28.473330Z"
        },
        "id": "ieS5SiQx4WSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e006afac-eef4-4795-e9c4-f562cf5921ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 6888.44it/s]\n"
          ]
        }
      ],
      "source": [
        "word2count, word2id, id2word = make_vocab(docs, min_count=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.754722Z",
          "start_time": "2022-02-19T14:33:30.752115Z"
        },
        "id": "cT1MRN1EJtx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39234248-c510-445b-a4bf-75880af4ed25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161,322\n"
          ]
        }
      ],
      "source": [
        "doc_len = sum(word2count.values()) # 문서 내 모든 단어의 개수 (단어별 등장 빈도의 총 합)\n",
        "print(f\"{doc_len:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:32.916830Z",
          "start_time": "2022-02-19T14:33:32.914355Z"
        },
        "id": "e_1MneB54WSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb7d579-8d45-42c0-ea83-20bf7e68fa25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# unique word : 5,662\n"
          ]
        }
      ],
      "source": [
        "print(f\"# unique word : {len(word2id):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHxtZqtk4WSm"
      },
      "source": [
        "### Dataset 클래스 구현\n",
        "- Skip-Gram 방식의 학습 데이터 셋(`Tuple(target_word, context_word)`)을 생성하는 `CustomDataset` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - docs: 문서 리스트\n",
        "        - word2id: 단어별 인덱스(wid) 사전\n",
        "        - window_size: Skip-Gram의 윈도우 사이즈\n",
        "    - 메소드\n",
        "        - `make_pair()`\n",
        "            - 문서를 단어로 쪼개고, 사전에 존재하는 단어들만 단어 인덱스로 변경\n",
        "            - Skip-gram 방식의 `(target_word, context_word)` 페어(tuple)들을 `pairs` 리스트에 담아 반환\n",
        "        - `__len__()`\n",
        "            - `pairs` 리스트의 개수 반환\n",
        "        - `__getitem__(index)`\n",
        "            - `pairs` 리스트를 인덱싱\n",
        "    - 주의 사항\n",
        "        - `nn.Module`를 부모 클래스로 상속 받음 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.111290Z",
          "start_time": "2022-02-19T14:33:38.104531Z"
        },
        "id": "UPiLcYCZ4WSm"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    문서 리스트를 받아 skip-gram 방식의 (target_word, context_word) 데이터 셋을 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, docs:List[str], word2id:Dict[str,int], window_size:int=5):\n",
        "        self.docs = docs\n",
        "        self.word2id = word2id\n",
        "        self.window_size = window_size\n",
        "        self.pairs = self.make_pair()\n",
        "    \n",
        "    def make_pair(self):\n",
        "        \"\"\"\n",
        "        (target, context) 형식의 Skip-gram pair 데이터 셋 생성 \n",
        "        \"\"\"\n",
        "        pairs = []\n",
        "        for doc in self.docs:\n",
        "          word_list = doc.split()\n",
        "          for i, word in enumerate(word_list):\n",
        "            if word in self.word2id : word_list[i] = self.word2id[word]\n",
        "\n",
        "          for i in range(len(word_list)):\n",
        "            if type(word_list[i]) != type(1):continue\n",
        "            min_index = max(0,i-self.window_size)\n",
        "            max_index = min(len(word_list)-1,i+self.window_size)\n",
        "\n",
        "            for j in range(min_index,max_index+1):\n",
        "              if type(word_list[j]) != type(1) : continue\n",
        "              if i == j : continue\n",
        "\n",
        "              pairs.append((word_list[i],word_list[j]))\n",
        "        \n",
        "        return pairs\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.945361Z",
          "start_time": "2022-02-19T14:33:38.385577Z"
        },
        "id": "YntOw2q94WSm"
      },
      "outputs": [],
      "source": [
        "dataset = CustomDataset(docs, word2id, window_size=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.949614Z",
          "start_time": "2022-02-19T14:33:38.946663Z"
        },
        "id": "-RpNbAjk4WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ea1ac5-6226-4c3d-d2db-0def1bf687eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024780"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:51.040595Z",
          "start_time": "2022-02-19T14:33:51.031473Z"
        },
        "id": "wTAwTjKk4WSn",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "976738bc-c9af-4f86-d569-901c6bc45760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(남모, 공주)\n",
            "(남모, 는)\n",
            "(남모, 신라)\n",
            "(남모, 공주)\n",
            "(공주, 남모)\n",
            "(공주, 는)\n",
            "(공주, 신라)\n",
            "(공주, 공주)\n",
            "(공주, 왕족)\n",
            "(는, 남모)\n",
            "(는, 공주)\n",
            "(는, 신라)\n",
            "(는, 공주)\n",
            "(는, 왕족)\n",
            "(신라, 남모)\n",
            "(신라, 공주)\n",
            "(신라, 는)\n",
            "(신라, 공주)\n",
            "(신라, 왕족)\n",
            "(공주, 남모)\n",
            "(공주, 공주)\n",
            "(공주, 는)\n",
            "(공주, 신라)\n",
            "(공주, 왕족)\n",
            "(왕족, 공주)\n",
            "(왕족, 는)\n",
            "(왕족, 신라)\n",
            "(왕족, 공주)\n",
            "(왕족, 공주)\n",
            "(공주, 왕족)\n",
            "(공주, 부여)\n",
            "(공주, 씨)\n",
            "(공주, 딸)\n",
            "(부여, 공주)\n",
            "(부여, 씨)\n",
            "(부여, 딸)\n",
            "(부여, 며)\n",
            "(씨, 공주)\n",
            "(씨, 부여)\n",
            "(씨, 딸)\n",
            "(씨, 며)\n",
            "(씨, 백제)\n",
            "(딸, 공주)\n",
            "(딸, 부여)\n",
            "(딸, 씨)\n",
            "(딸, 며)\n",
            "(딸, 백제)\n",
            "(며, 부여)\n",
            "(며, 씨)\n",
            "(며, 딸)\n",
            "(며, 백제)\n",
            "(며, 였)\n",
            "(백제, 씨)\n",
            "(백제, 딸)\n",
            "(백제, 며)\n",
            "(백제, 였)\n",
            "(백제, 다)\n",
            "(였, 며)\n",
            "(였, 백제)\n",
            "(였, 다)\n",
            "(였, 인)\n",
            "(였, 준정)\n",
            "(다, 백제)\n",
            "(다, 였)\n",
            "(다, 인)\n",
            "(다, 준정)\n",
            "(인, 였)\n",
            "(인, 다)\n",
            "(인, 준정)\n",
            "(인, 신라)\n",
            "(준정, 였)\n",
            "(준정, 다)\n",
            "(준정, 인)\n",
            "(준정, 신라)\n",
            "(준정, 초대)\n",
            "(신라, 인)\n",
            "(신라, 준정)\n",
            "(신라, 초대)\n",
            "(신라, 여성)\n",
            "(신라, 화랑)\n",
            "(초대, 준정)\n",
            "(초대, 신라)\n",
            "(초대, 여성)\n",
            "(초대, 화랑)\n",
            "(초대, 였)\n",
            "(초대, 다)\n",
            "(여성, 신라)\n",
            "(여성, 초대)\n",
            "(여성, 화랑)\n",
            "(여성, 였)\n",
            "(여성, 다)\n",
            "(화랑, 신라)\n",
            "(화랑, 초대)\n",
            "(화랑, 여성)\n",
            "(화랑, 였)\n",
            "(화랑, 다)\n",
            "(화랑, 준정)\n",
            "(였, 초대)\n",
            "(였, 여성)\n",
            "(였, 화랑)\n"
          ]
        }
      ],
      "source": [
        "# verify (target word, context word)\n",
        "for i, pair in enumerate(dataset):\n",
        "    if i==100:\n",
        "        break\n",
        "    print(f\"({id2word[pair[0]]}, {id2word[pair[1]]})\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Z50-Dr4WSn"
      },
      "source": [
        "### 위에서 생성한 `dataset`으로 DataLoader  객체 생성\n",
        "- `DataLoader` 클래스로 `train_dataloader`객체를 생성하라. \n",
        "    - 생성자 매개변수와 값\n",
        "        - dataset = 위에서 생성한 dataset\n",
        "        - batch_size = 64\n",
        "        - shuffle = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.645176Z",
          "start_time": "2022-02-19T14:34:02.642780Z"
        },
        "id": "GXcAvFB14WSn"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(dataset, batch_size = 64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.777322Z",
          "start_time": "2022-02-19T14:34:02.774335Z"
        },
        "id": "4Yfcwi_14WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed48ac76-e3d2-4244-f2af-df4466d5c37d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16013"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTs16gsU4WSn"
      },
      "source": [
        "### Negative Sampling 함수 구현\n",
        "- Skip-Gram은 복잡도를 줄이기 위한 방법으로 negative sampling을 사용한다. \n",
        "- `sample_table`이 다음과 같이 주어졌을 때, sample_table에서 랜덤으로 값을 뽑아 (batch_size, n_neg_sample) shape의 matrix를 반환하는 `get_neg_v_negative_sampling()`함수를 구현하라. \n",
        "- Sample Table은 negative distribution을 따른다. \n",
        "    - [negative distribution 설명](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#How-are-negative-samples-drawn?)\n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - batch_size : 배치 사이즈, matrix의 row 개수 \n",
        "        - n_neg_sample : negative sample의 개수, matrix의 column 개수\n",
        "    - 반환값 \n",
        "        - neg_v : 추출된 negative sample (2차원의 리스트)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.397509Z",
          "start_time": "2022-02-19T14:34:11.386389Z"
        },
        "id": "PUqIB6dH4WSn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# negative sample을 추출할 sample table 생성 (해당 코드를 참고)\n",
        "sample_table = []\n",
        "sample_table_size = doc_len\n",
        "\n",
        "# noise distribution 생성\n",
        "alpha = 3/4\n",
        "frequency_list = np.array(list(word2count.values())) ** alpha\n",
        "Z = sum(frequency_list)\n",
        "ratio = frequency_list/Z\n",
        "negative_sample_dist = np.round(ratio*sample_table_size)\n",
        "\n",
        "for wid, c in enumerate(negative_sample_dist):\n",
        "    sample_table.extend([wid]*int(c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.508414Z",
          "start_time": "2022-02-19T14:34:11.505464Z"
        },
        "id": "Wdu8qK8x4WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2768f34b-d93c-4831-c33d-940c637581d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "161159"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "len(sample_table)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2count.values())\n",
        "print(negative_sample_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD3NPB8ZWDdC",
        "outputId": "55bc9355-be2d-4345-9724-9e37ed45f61d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_values([6, 52, 5895, 27, 15, 20, 189, 70, 486, 8, 789, 5722, 1002, 9, 52, 88, 160, 5, 10, 16, 3711, 30, 30, 686, 1744, 1589, 9, 11, 318, 11, 62, 404, 77, 250, 5, 11, 207, 63, 10, 373, 17, 46, 46, 62, 21, 64, 200, 769, 131, 27, 979, 1120, 1951, 5, 2364, 38, 158, 364, 271, 123, 136, 13, 66, 17, 14, 10, 31, 107, 9, 94, 21, 139, 55, 10, 51, 10, 244, 5, 6, 473, 10, 5, 94, 2488, 74, 563, 370, 292, 8, 21, 477, 692, 2384, 39, 158, 314, 964, 10, 100, 36, 222, 98, 13, 18, 15, 13, 163, 47, 75, 5, 15, 7, 51, 8, 272, 108, 5, 173, 46, 41, 161, 156, 5, 15, 30, 21, 62, 193, 115, 63, 52, 8, 128, 180, 45, 17, 9, 47, 19, 1006, 5, 134, 73, 232, 676, 10, 10, 108, 131, 9, 118, 7, 16, 8, 28, 15, 112, 22, 22, 5, 43, 25, 107, 191, 5, 14, 8, 57, 75, 54, 6, 54, 9, 5, 20, 21, 35, 15, 7, 57, 55, 26, 13, 19, 5, 9, 30, 5, 11, 138, 259, 26, 53, 11, 59, 6, 9, 8, 12, 10, 17, 7, 51, 313, 13, 9, 15, 8, 22, 28, 9, 21, 108, 6, 97, 26, 17, 103, 6, 54, 38, 104, 702, 6, 14, 6, 21, 12, 9, 6, 26, 9, 61, 7, 74, 11, 16, 16, 29, 76, 23, 331, 11, 9, 34, 7, 6, 8, 35, 6, 7, 104, 34, 39, 69, 38, 39, 49, 30, 167, 10, 14, 5, 6, 41, 8, 12, 18, 9, 55, 8, 36, 48, 11, 18, 86, 16, 50, 298, 39, 14, 393, 24, 16, 82, 297, 382, 13, 60, 185, 20, 41, 14, 39, 90, 5, 148, 116, 35, 16, 72, 11, 59, 11, 47, 35, 14, 254, 42, 140, 11, 26, 24, 44, 61, 19, 13, 31, 16, 7, 10, 32, 159, 336, 166, 43, 39, 281, 34, 227, 55, 34, 65, 11, 34, 8, 14, 314, 17, 10, 23, 127, 12, 10, 106, 17, 19, 24, 14, 53, 35, 50, 26, 70, 99, 321, 12, 22, 53, 14, 120, 9, 116, 83, 34, 11, 25, 32, 53, 6, 22, 159, 35, 5, 63, 7, 80, 25, 8, 76, 9, 84, 47, 33, 12, 6, 67, 159, 51, 66, 9, 7, 19, 35, 23, 22, 33, 33, 10, 59, 12, 31, 37, 155, 40, 6, 534, 806, 22, 41, 8, 62, 100, 7, 62, 6, 26, 45, 5, 6, 18, 18, 9, 18, 6, 47, 5, 47, 5, 6, 10, 568, 43, 53, 48, 17, 66, 8, 279, 80, 30, 6, 92, 10, 25, 116, 26, 8, 46, 281, 92, 28, 7, 5, 15, 19, 6, 7, 32, 7, 13, 11, 6, 49, 12, 32, 36, 13, 43, 16, 61, 23, 11, 10, 50, 7, 11, 38, 10, 85, 6, 10, 46, 23, 5, 23, 17, 11, 46, 26, 17, 32, 47, 24, 67, 21, 17, 164, 15, 15, 7, 35, 124, 19, 66, 7, 6, 20, 56, 39, 7, 8, 103, 5, 8, 37, 13, 118, 323, 16, 118, 15, 7, 9, 127, 11, 61, 21, 97, 46, 5, 6, 8, 7, 311, 8, 35, 177, 5, 14, 54, 34, 31, 72, 5, 104, 67, 89, 22, 30, 35, 223, 75, 48, 169, 12, 25, 22, 19, 30, 56, 9, 17, 11, 9, 15, 101, 71, 72, 7, 125, 13, 8, 5, 37, 263, 106, 8, 55, 157, 73, 44, 5, 7, 41, 13, 64, 33, 46, 11, 5, 42, 24, 21, 26, 5, 44, 42, 60, 27, 36, 17, 15, 11, 48, 13, 9, 6, 15, 71, 12, 6, 58, 7, 19, 7, 98, 43, 130, 12, 46, 76, 32, 126, 44, 307, 167, 36, 7, 14, 88, 43, 11, 15, 7, 43, 7, 211, 5, 13, 11, 16, 5, 18, 12, 70, 85, 13, 19, 62, 21, 16, 24, 22, 14, 66, 6, 76, 8, 8, 11, 43, 9, 26, 6, 40, 12, 38, 7, 45, 5, 7, 71, 5, 27, 111, 54, 7, 5, 52, 36, 10, 21, 8, 99, 17, 30, 19, 88, 49, 22, 5, 14, 19, 16, 7, 5, 74, 28, 71, 129, 14, 10, 8, 5, 5, 12, 7, 26, 7, 10, 5, 242, 16, 13, 10, 6, 6, 60, 14, 44, 8, 26, 23, 6, 14, 23, 18, 35, 18, 6, 14, 20, 44, 15, 30, 9, 5, 8, 12, 29, 9, 15, 16, 18, 11, 7, 16, 112, 7, 70, 183, 8, 37, 55, 6, 7, 10, 14, 16, 6, 6, 5, 31, 42, 5, 5, 6, 27, 8, 54, 36, 15, 8, 85, 36, 9, 36, 51, 6, 43, 144, 43, 10, 5, 34, 22, 44, 10, 97, 200, 11, 29, 50, 17, 37, 46, 6, 5, 33, 5, 94, 7, 6, 16, 456, 148, 37, 14, 51, 12, 76, 29, 17, 6, 7, 6, 6, 27, 5, 10, 5, 6, 8, 9, 13, 66, 9, 5, 11, 32, 14, 5, 51, 6, 5, 8, 18, 27, 11, 9, 42, 24, 10, 32, 37, 63, 24, 8, 9, 48, 52, 8, 21, 122, 7, 39, 9, 154, 24, 13, 5, 10, 6, 24, 11, 16, 20, 5, 11, 223, 10, 39, 12, 10, 53, 33, 7, 31, 12, 41, 22, 16, 43, 31, 29, 12, 49, 27, 17, 27, 24, 5, 27, 89, 15, 13, 9, 30, 8, 6, 19, 30, 5, 22, 79, 19, 5, 55, 14, 9, 39, 6, 63, 8, 30, 25, 5, 28, 24, 10, 54, 9, 6, 14, 25, 11, 44, 36, 30, 5, 6, 14, 13, 17, 7, 16, 20, 28, 37, 20, 30, 44, 23, 43, 5, 7, 12, 5, 27, 10, 103, 12, 6, 13, 8, 9, 7, 18, 46, 97, 42, 31, 15, 5, 14, 40, 41, 18, 96, 32, 8, 11, 87, 19, 18, 8, 145, 5, 8, 8, 5, 28, 13, 15, 12, 42, 5, 34, 10, 21, 45, 16, 30, 7, 46, 36, 24, 27, 22, 63, 27, 12, 21, 12, 36, 54, 8, 5, 6, 24, 5, 11, 14, 7, 20, 8, 24, 38, 15, 74, 5, 6, 6, 11, 65, 73, 28, 56, 6, 8, 29, 13, 11, 115, 11, 5, 7, 5, 7, 18, 44, 54, 59, 6, 66, 9, 16, 5, 15, 53, 14, 8, 71, 12, 71, 5, 22, 26, 38, 31, 7, 19, 6, 7, 7, 168, 9, 68, 10, 118, 18, 17, 5, 25, 100, 29, 12, 18, 16, 5, 6, 6, 8, 63, 19, 23, 49, 41, 32, 122, 13, 6, 18, 18, 77, 20, 21, 10, 13, 14, 7, 10, 17, 28, 16, 18, 13, 14, 20, 13, 84, 30, 5, 11, 16, 39, 133, 43, 50, 16, 18, 5, 11, 21, 20, 5, 34, 5, 5, 18, 12, 34, 9, 35, 32, 25, 7, 5, 5, 24, 11, 27, 14, 47, 25, 6, 20, 14, 8, 5, 8, 87, 21, 29, 15, 7, 6, 21, 31, 41, 126, 10, 211, 6, 28, 7, 19, 18, 120, 28, 35, 8, 5, 35, 9, 16, 10, 26, 65, 5, 59, 8, 9, 68, 61, 49, 60, 36, 25, 18, 23, 35, 63, 33, 18, 5, 9, 30, 8, 48, 21, 15, 33, 32, 11, 9, 95, 48, 54, 15, 52, 24, 14, 105, 5, 9, 10, 11, 12, 9, 20, 23, 16, 39, 48, 43, 41, 9, 6, 7, 8, 7, 5, 10, 15, 5, 5, 5, 5, 17, 6, 15, 14, 11, 9, 94, 130, 7, 18, 149, 11, 25, 16, 16, 10, 12, 22, 7, 26, 23, 13, 22, 6, 5, 5, 10, 6, 13, 45, 14, 18, 8, 87, 35, 10, 17, 5, 17, 6, 19, 23, 14, 25, 35, 51, 27, 18, 43, 21, 7, 5, 12, 13, 5, 5, 9, 13, 84, 193, 25, 8, 91, 10, 11, 6, 58, 54, 10, 22, 24, 14, 21, 14, 40, 9, 33, 19, 7, 9, 40, 11, 11, 7, 7, 53, 17, 21, 42, 7, 9, 19, 57, 23, 24, 8, 48, 63, 24, 93, 22, 15, 49, 5, 8, 16, 60, 47, 58, 11, 8, 12, 19, 50, 278, 26, 18, 6, 11, 17, 86, 6, 27, 68, 7, 7, 6, 62, 12, 27, 30, 5, 21, 10, 17, 45, 7, 114, 16, 6, 19, 20, 6, 13, 7, 6, 7, 20, 6, 29, 9, 10, 17, 44, 33, 5, 11, 5, 11, 10, 25, 36, 11, 12, 6, 6, 36, 22, 36, 10, 6, 28, 6, 12, 70, 5, 9, 5, 59, 18, 6, 19, 6, 15, 29, 38, 15, 8, 7, 58, 5, 7, 26, 45, 16, 8, 6, 21, 6, 34, 14, 59, 20, 25, 5, 8, 5, 21, 24, 10, 19, 6, 8, 44, 6, 13, 7, 8, 10, 13, 11, 56, 55, 6, 15, 6, 25, 34, 96, 13, 6, 16, 16, 39, 32, 22, 13, 19, 47, 5, 8, 5, 16, 160, 62, 31, 23, 61, 5, 5, 7, 9, 61, 30, 11, 8, 32, 10, 26, 22, 59, 31, 6, 34, 11, 11, 57, 5, 15, 8, 15, 14, 78, 35, 62, 15, 22, 15, 49, 40, 17, 16, 105, 16, 15, 12, 34, 6, 30, 26, 9, 30, 48, 26, 8, 14, 10, 32, 54, 7, 17, 5, 6, 7, 6, 58, 90, 9, 9, 11, 14, 5, 5, 31, 6, 20, 8, 14, 21, 160, 6, 52, 16, 6, 11, 65, 24, 5, 56, 12, 39, 9, 5, 11, 42, 6, 25, 37, 5, 21, 71, 47, 18, 25, 5, 25, 45, 9, 7, 50, 7, 23, 9, 8, 5, 19, 9, 92, 27, 29, 12, 17, 30, 5, 7, 33, 10, 51, 22, 9, 37, 11, 77, 5, 12, 11, 5, 10, 8, 11, 20, 9, 207, 15, 8, 20, 6, 10, 5, 22, 157, 32, 13, 65, 7, 5, 7, 31, 7, 6, 9, 26, 155, 16, 22, 16, 5, 25, 46, 13, 99, 12, 9, 12, 14, 16, 21, 25, 6, 7, 44, 10, 24, 55, 10, 81, 22, 22, 13, 7, 58, 13, 65, 61, 9, 6, 73, 281, 109, 42, 35, 8, 16, 28, 16, 10, 19, 22, 24, 12, 134, 13, 45, 5, 7, 9, 827, 19, 18, 32, 261, 15, 37, 10, 119, 9, 8, 8, 24, 7, 13, 27, 12, 8, 15, 13, 6, 6, 10, 51, 7, 6, 13, 5, 5, 23, 25, 8, 6, 5, 5, 18, 48, 25, 65, 26, 7, 30, 8, 29, 10, 8, 11, 7, 6, 6, 9, 15, 5, 5, 11, 8, 17, 6, 26, 7, 15, 27, 11, 7, 23, 16, 38, 26, 11, 11, 25, 27, 8, 9, 102, 6, 27, 9, 14, 7, 15, 8, 24, 14, 15, 6, 16, 9, 16, 15, 27, 6, 22, 18, 689, 30, 12, 9, 16, 16, 8, 7, 24, 28, 9, 61, 38, 29, 13, 8, 10, 14, 53, 39, 8, 13, 29, 22, 36, 59, 7, 16, 8, 18, 40, 14, 6, 35, 49, 22, 41, 43, 12, 9, 33, 14, 53, 8, 16, 7, 22, 45, 29, 125, 19, 17, 25, 5, 5, 8, 26, 7, 21, 13, 8, 6, 22, 16, 10, 6, 8, 21, 31, 27, 9, 7, 8, 35, 95, 62, 89, 41, 11, 20, 10, 42, 25, 14, 5, 6, 5, 11, 5, 9, 5, 32, 25, 17, 12, 121, 8, 250, 19, 11, 25, 8, 46, 71, 32, 5, 29, 15, 24, 9, 11, 12, 7, 45, 5, 8, 16, 11, 7, 17, 7, 5, 87, 12, 7, 89, 21, 10, 9, 69, 8, 5, 8, 12, 7, 15, 52, 6, 11, 6, 13, 17, 41, 6, 23, 13, 6, 12, 18, 53, 21, 13, 49, 9, 8, 44, 6, 11, 9, 15, 7, 42, 17, 7, 15, 24, 22, 48, 38, 23, 12, 35, 8, 11, 7, 10, 10, 9, 6, 39, 12, 39, 7, 64, 16, 8, 6, 5, 6, 7, 9, 7, 60, 9, 6, 14, 50, 5, 9, 9, 38, 6, 26, 12, 51, 10, 7, 29, 103, 8, 6, 10, 99, 10, 15, 9, 32, 5, 5, 21, 28, 5, 6, 30, 13, 6, 79, 14, 8, 103, 8, 15, 22, 15, 5, 9, 5, 7, 22, 10, 6, 10, 23, 5, 9, 11, 5, 15, 14, 8, 6, 10, 8, 50, 7, 10, 26, 5, 25, 6, 6, 19, 15, 5, 7, 10, 14, 7, 33, 17, 7, 8, 8, 8, 13, 18, 6, 216, 5, 18, 22, 33, 12, 17, 5, 5, 8, 39, 38, 10, 92, 8, 5, 16, 31, 22, 5, 9, 10, 38, 10, 8, 5, 10, 5, 15, 17, 134, 15, 15, 39, 14, 29, 15, 6, 82, 11, 25, 5, 58, 10, 13, 23, 67, 21, 18, 11, 19, 5, 20, 40, 11, 17, 7, 23, 18, 5, 8, 6, 15, 11, 9, 10, 10, 5, 10, 15, 37, 5, 21, 77, 11, 9, 33, 7, 56, 63, 11, 7, 20, 6, 18, 10, 11, 7, 9, 13, 22, 14, 30, 77, 16, 25, 10, 7, 5, 7, 15, 28, 11, 30, 6, 16, 15, 32, 35, 11, 7, 21, 5, 5, 21, 82, 29, 9, 8, 8, 5, 8, 9, 5, 8, 13, 5, 6, 15, 9, 21, 5, 15, 40, 33, 6, 21, 25, 77, 5, 8, 37, 12, 21, 5, 9, 16, 14, 7, 13, 15, 14, 22, 25, 101, 10, 5, 47, 25, 6, 14, 5, 14, 12, 38, 24, 12, 9, 26, 7, 9, 16, 11, 11, 6, 18, 26, 185, 8, 7, 7, 13, 17, 9, 5, 5, 14, 13, 21, 8, 6, 12, 12, 13, 8, 38, 10, 24, 5, 8, 11, 22, 5, 5, 12, 15, 12, 5, 7, 23, 11, 21, 17, 22, 29, 17, 9, 5, 8, 15, 17, 15, 10, 66, 12, 22, 10, 21, 86, 22, 69, 8, 11, 38, 33, 8, 5, 13, 5, 22, 9, 7, 54, 9, 11, 9, 12, 17, 13, 168, 15, 16, 14, 11, 15, 26, 8, 23, 128, 13, 5, 5, 14, 11, 15, 8, 29, 13, 99, 26, 5, 41, 16, 23, 24, 6, 47, 14, 17, 10, 9, 11, 5, 10, 18, 8, 5, 16, 5, 34, 15, 16, 17, 9, 5, 129, 31, 6, 21, 6, 54, 8, 5, 85, 5, 28, 5, 18, 14, 16, 7, 82, 5, 5, 30, 17, 28, 5, 11, 12, 13, 14, 8, 11, 11, 20, 12, 13, 7, 6, 6, 16, 5, 9, 6, 11, 45, 24, 40, 38, 8, 5, 40, 10, 9, 13, 16, 136, 6, 47, 5, 5, 7, 20, 6, 12, 30, 56, 12, 15, 8, 134, 6, 75, 18, 37, 6, 58, 6, 5, 86, 22, 7, 18, 16, 6, 27, 11, 5, 11, 6, 18, 9, 104, 18, 22, 44, 7, 5, 5, 5, 10, 5, 5, 6, 5, 5, 5, 15, 16, 16, 10, 8, 12, 9, 10, 6, 10, 27, 15, 8, 14, 10, 5, 5, 28, 12, 7, 5, 11, 27, 16, 21, 7, 12, 8, 40, 10, 22, 6, 8, 6, 11, 53, 37, 22, 8, 9, 11, 8, 37, 10, 8, 22, 8, 8, 29, 91, 17, 10, 10, 11, 55, 43, 11, 8, 8, 7, 10, 16, 28, 30, 10, 5, 29, 47, 11, 82, 5, 86, 45, 9, 6, 9, 22, 6, 20, 12, 22, 11, 5, 14, 34, 77, 10, 14, 38, 10, 21, 5, 25, 45, 25, 6, 6, 5, 18, 6, 11, 26, 16, 7, 11, 18, 19, 14, 5, 9, 5, 7, 18, 62, 5, 20, 7, 6, 5, 9, 12, 5, 7, 12, 9, 11, 6, 14, 9, 13, 14, 19, 16, 52, 38, 27, 5, 7, 13, 6, 6, 5, 5, 7, 5, 5, 8, 9, 23, 12, 5, 11, 5, 34, 23, 29, 10, 5, 10, 7, 12, 24, 23, 10, 8, 15, 21, 7, 5, 5, 19, 6, 25, 5, 20, 6, 5, 5, 13, 23, 13, 10, 15, 6, 5, 28, 14, 9, 6, 7, 8, 7, 5, 34, 6, 8, 34, 25, 8, 9, 64, 41, 6, 29, 6, 6, 18, 43, 37, 8, 9, 31, 13, 7, 5, 7, 5, 10, 15, 9, 51, 7, 114, 12, 6, 9, 5, 19, 10, 6, 9, 5, 9, 10, 17, 7, 9, 11, 14, 13, 8, 11, 7, 13, 7, 9, 9, 8, 10, 9, 17, 13, 7, 5, 25, 15, 7, 6, 14, 7, 6, 27, 8, 17, 10, 17, 11, 17, 24, 6, 12, 31, 5, 17, 10, 18, 12, 11, 12, 6, 38, 11, 8, 16, 9, 24, 19, 11, 5, 16, 11, 7, 20, 41, 9, 14, 7, 5, 20, 22, 27, 6, 15, 8, 17, 75, 7, 7, 124, 30, 8, 5, 5, 31, 12, 6, 18, 7, 6, 5, 6, 9, 5, 6, 11, 15, 20, 10, 8, 46, 7, 7, 8, 39, 12, 8, 14, 27, 6, 7, 7, 38, 50, 23, 7, 6, 5, 7, 15, 19, 234, 164, 47, 25, 25, 137, 5, 8, 7, 8, 13, 11, 6, 7, 5, 7, 7, 5, 5, 7, 18, 13, 5, 18, 17, 5, 10, 23, 22, 6, 27, 181, 13, 25, 8, 56, 5, 32, 10, 5, 9, 8, 13, 27, 13, 5, 11, 26, 18, 7, 5, 17, 5, 8, 9, 16, 13, 20, 18, 7, 8, 18, 6, 11, 5, 7, 15, 7, 6, 15, 16, 8, 11, 15, 18, 5, 6, 16, 18, 8, 11, 13, 6, 7, 7, 18, 5, 8, 8, 5, 12, 11, 20, 9, 5, 7, 13, 7, 5, 20, 5, 8, 14, 8, 10, 12, 12, 15, 5, 6, 15, 9, 9, 5, 11, 6, 9, 13, 12, 13, 6, 8, 7, 6, 9, 10, 34, 29, 6, 29, 64, 7, 11, 23, 9, 16, 27, 8, 12, 10, 9, 14, 10, 6, 22, 13, 6, 15, 22, 15, 14, 9, 6, 23, 7, 14, 70, 6, 11, 8, 5, 10, 6, 20, 6, 7, 13, 8, 8, 6, 9, 5, 5, 20, 5, 5, 24, 7, 6, 7, 6, 5, 32, 5, 44, 15, 5, 15, 5, 11, 9, 14, 7, 7, 6, 40, 7, 16, 12, 15, 12, 10, 19, 10, 20, 5, 7, 5, 29, 17, 19, 7, 9, 11, 8, 6, 17, 5, 6, 9, 18, 11, 23, 27, 9, 9, 10, 6, 8, 16, 49, 16, 9, 11, 6, 16, 14, 6, 5, 13, 7, 5, 7, 8, 22, 7, 7, 8, 5, 14, 12, 17, 10, 5, 8, 27, 11, 17, 11, 11, 5, 30, 5, 10, 11, 9, 50, 34, 6, 12, 8, 5, 31, 14, 11, 9, 6, 10, 6, 37, 155, 6, 6, 6, 13, 6, 45, 7, 14, 12, 6, 6, 7, 6, 33, 6, 5, 10, 46, 15, 14, 13, 10, 13, 9, 8, 7, 6, 32, 30, 5, 6, 6, 6, 10, 5, 5, 6, 6, 15, 7, 7, 10, 6, 17, 25, 8, 6, 12, 12, 6, 6, 10, 5, 17, 14, 6, 11, 10, 5, 7, 9, 14, 12, 11, 16, 6, 8, 7, 5, 15, 14, 6, 8, 5, 14, 6, 8, 7, 17, 10, 31, 5, 6, 20, 8, 10, 5, 5, 8, 5, 7, 5, 22, 8, 12, 8, 5, 8, 5, 5, 6, 12, 24, 8, 6, 16, 19, 5, 8, 13, 8, 8, 7, 7, 6, 32, 7, 13, 6, 6, 6, 12, 5, 10, 5, 26, 10, 5, 27, 12, 64, 18, 5, 11, 27, 9, 5, 12, 9, 9, 5, 7, 5, 6, 34, 31, 13, 10, 10, 9, 6, 9, 8, 12, 13, 10, 33, 5, 7, 8, 5, 6, 16, 11, 7, 18, 14, 8, 41, 9, 5, 5, 6, 12, 36, 5, 22, 29, 6, 28, 9, 5, 10, 9, 7, 16, 5, 6, 9, 21, 21, 13, 28, 15, 12, 13, 7, 6, 20, 21, 8, 21, 7, 6, 15, 6, 32, 10, 6, 8, 8, 12, 11, 22, 19, 31, 5, 5, 9, 5, 7, 8, 10, 7, 7, 15, 10, 20, 5, 5, 27, 10, 13, 14, 5, 11, 6, 9, 10, 7, 8, 12, 6, 13, 13, 44, 9, 5, 5, 61, 9, 11, 5, 10, 10, 12, 5, 9, 5, 6, 7, 9, 15, 15, 8, 6, 14, 16, 9, 15, 6, 17, 16, 12, 7, 16, 6, 7, 6, 6, 5, 5, 6, 6, 7, 5, 7, 15, 6, 6, 5, 10, 30, 7, 66, 11, 8, 7, 30, 6, 5, 9, 24, 5, 7, 33, 10, 14, 6, 7, 5, 11, 7, 15, 9, 18, 10, 5, 11, 10, 7, 91, 52, 6, 29, 5, 13, 10, 7, 6, 8, 14, 9, 6, 6, 35, 10, 10, 6, 7, 8, 20, 5, 12, 11, 5, 5, 12, 11, 10, 12, 7, 6, 11, 30, 12, 7, 18, 15, 46, 6, 24, 13, 9, 15, 7, 8, 26, 5, 5, 6, 14, 7, 8, 7, 6, 6, 5, 13, 12, 5, 24, 5, 5, 26, 24, 8, 9, 19, 9, 7, 6, 5, 5, 13, 12, 9, 8, 5, 9, 8, 15, 9, 7, 8, 6, 15, 7, 5, 8, 5, 11, 11, 9, 38, 8, 13, 6, 11, 7, 13, 6, 5, 6, 8, 15, 5, 7, 6, 13, 7, 6, 15, 12, 7, 7, 8, 35, 5, 18, 6, 8, 11, 6, 5, 13, 6, 7, 84, 12, 5, 19, 5, 5, 5, 5, 14, 5, 6, 6, 11, 9, 5, 18, 32, 19, 8, 9, 5, 27, 6, 8, 11, 9, 5, 19, 15, 7, 6, 5, 6, 6, 7, 5, 18, 8, 9, 7, 6, 7, 14, 5, 8, 7, 17, 5, 13, 8, 9, 12, 9, 5, 7, 10, 6, 23, 5, 18, 6, 8, 5, 37, 5, 9, 9, 10, 14, 6, 8, 40, 10, 17, 9, 5, 7, 30, 6, 5, 17, 18, 7, 5, 18, 5, 7, 7, 6, 12, 6, 5, 8, 10, 12, 32, 7, 5, 8, 64, 5, 5, 12, 25, 8, 13, 6, 16, 9, 18, 30, 23, 5, 9, 12, 19, 6, 5, 8, 8, 20, 7, 5, 9, 5, 10, 6, 8, 14, 23, 20, 7, 7, 16, 8, 5, 7, 15, 21, 8, 15, 7, 6, 7, 8, 11, 11, 5, 10, 10, 5, 5, 10, 6, 5, 5, 19, 5, 9, 18, 21, 16, 6, 5, 8, 14, 8, 10, 10, 5, 6, 8, 6, 6, 26, 13, 14, 5, 7, 5, 12, 20, 11, 15, 12, 15, 8, 8, 6, 23, 18, 5, 5, 9, 7, 9, 6, 10, 11, 6, 6, 17, 7, 6, 6, 38, 5, 8, 9, 6, 7, 7, 7, 6, 7, 17, 9, 26, 20, 14, 17, 52, 33, 66, 56, 15, 17, 15, 8, 14, 8, 8, 38, 8, 5, 24, 5, 5, 7, 6, 10, 8, 5, 5, 5, 7, 13, 10, 7, 8, 5, 12, 25, 6, 9, 12, 8, 8, 18, 12, 13, 6, 5, 6, 8, 5, 6, 6, 7, 7, 7, 7, 5, 9, 5, 12, 8, 6, 5, 21, 7, 5, 5, 5, 11, 9, 12, 16, 7, 16, 9, 19, 7, 9, 5, 13, 81, 7, 10, 10, 24, 5, 5, 7, 11, 16, 5, 8, 6, 5, 9, 6, 13, 23, 6, 6, 6, 6, 7, 5, 7, 8, 6, 9, 13, 5, 11, 7, 8, 18, 9, 12, 8, 28, 10, 7, 7, 13, 16, 36, 5, 10, 10, 6, 6, 6, 7, 7, 6, 5, 7, 25, 6, 5, 9, 11, 8, 10, 11, 11, 17, 8, 5, 5, 5, 7, 11, 32, 6, 12, 9, 12, 6, 5, 8, 8, 8, 5, 5, 5, 5, 9, 5, 6, 5, 16, 5, 8, 9, 5, 5, 24, 15, 6, 9, 6, 5, 14, 8, 5, 13, 36, 5, 5, 14, 9, 9, 37, 9, 11, 7, 13, 13, 6, 11, 7, 25, 5, 9, 15, 19, 5, 7, 7, 8, 8, 6, 5, 34, 7, 5, 7, 10, 8, 9, 5, 5, 13, 9, 9, 19, 5, 5, 8, 5, 5, 11, 23, 15, 6, 11, 7, 7, 10, 13, 5, 8, 9, 28, 8, 8, 25, 8, 15, 5, 18, 7, 10, 5, 6, 24, 6, 6, 11, 7, 9, 5, 35, 5, 17, 18, 34, 22, 21, 6, 5, 8, 5, 5, 5, 9, 6, 7, 6, 5, 9, 11, 7, 5, 8, 18, 5, 5, 5, 13, 10, 33, 5, 5, 11, 5, 16, 44, 5, 10, 5, 5, 8, 6, 7, 6, 8, 19, 5, 11, 6, 7, 7, 9, 7, 7, 9, 6, 5, 8, 6, 5, 5, 5, 5, 5, 21, 5, 12, 6, 6, 15, 13, 103, 10, 8, 5, 6, 8, 10, 10, 5, 5, 8, 9, 10, 7, 5, 7, 6, 26, 9, 7, 6, 13, 6, 13, 5, 12, 8, 10, 6, 10, 8, 11, 14, 7, 6, 10, 5, 8, 6, 6, 61, 9, 5, 6, 11, 5, 7, 15, 5, 5, 6, 6, 16, 8, 7, 5, 5, 5, 5, 20, 5, 9, 15, 7, 9, 8, 7, 7, 5, 6, 5, 6, 55, 86, 6, 33, 15, 5, 6, 8, 5, 14, 8, 6, 6, 5, 11, 22, 9, 8, 21, 6, 8, 19, 11, 5, 10, 5, 5, 14, 7, 13, 5, 11, 8, 9, 6, 7, 11, 6, 6, 9, 7, 7, 5, 5, 7, 6, 6, 8, 5, 13, 6, 6, 77, 5, 5, 15, 8, 7, 8, 13, 12, 6, 23, 12, 19, 6, 11, 6, 7, 5, 12, 6, 20, 8, 5, 7, 6, 17, 6, 8, 5, 36, 6, 11, 5, 5, 5, 12, 13, 5, 6, 11, 9, 5, 5, 13, 5, 6, 6, 5, 7, 8, 7, 8, 5, 8, 6, 8, 6, 16, 7, 5, 33, 78, 9, 7, 6, 6, 8, 26, 11, 9, 31, 6, 5, 6, 5, 5, 9, 16, 14, 5, 7, 6, 9, 6, 5, 13, 5, 9, 5, 7, 8, 6, 22, 13, 14, 6, 15, 41, 8, 38, 5, 5, 14, 6, 21, 7, 11, 6, 10, 5, 5, 115, 6, 45, 38, 36, 6, 6, 12, 11, 6, 6, 18, 17, 9, 6, 8, 5, 14, 8, 16, 15, 12, 40, 9, 8, 24, 8, 5, 10, 7, 5, 10, 5, 10, 20, 5, 8, 5, 13, 9, 10, 8, 6, 6, 21, 5, 5, 6, 6, 5, 5, 16, 13, 5, 6, 5, 19, 5, 5, 5, 9, 8, 7, 6, 30, 38, 5, 21, 15, 14, 11, 12, 19, 6, 6, 21, 7, 6, 6, 10, 9, 7, 5, 5, 5, 6, 6, 6, 8, 8, 26, 5, 7, 28, 5, 6, 5, 5, 5, 5, 6, 5, 8, 14, 8, 9, 12, 10, 12, 5, 10, 6, 27, 7, 12, 15, 6, 6, 9, 9, 8, 15, 44, 10, 21, 8, 9, 8, 6, 5, 7, 10, 7, 14, 11, 213, 14, 29, 5, 7, 5, 7, 13, 5, 6, 7, 6, 6, 5, 7, 9, 5, 8, 7, 6, 61, 8, 169, 22, 16, 7, 15, 32, 25, 7, 49, 9, 6, 8, 14, 10, 13, 19, 10, 5, 8, 5, 72, 18, 6, 19, 12, 14, 5, 10, 5, 5, 7, 13, 8, 6, 26, 28, 25, 11, 9, 13, 6, 7, 13, 13, 22, 5, 5, 5, 10, 12, 8, 34, 10, 10, 5, 8, 14, 6, 15, 6, 10, 8, 6, 6, 21, 5, 45, 5, 5, 16, 18, 14, 5, 35, 62, 5, 6, 18, 8, 14, 14, 10, 11, 5, 6, 5, 6, 5, 8, 6, 5, 12, 11, 9, 5, 12, 7, 8, 5, 5, 7, 7, 6, 21, 23, 6, 12, 5, 5, 7, 5, 8, 14, 5, 6, 6, 5, 47, 12, 13, 6, 6, 6, 5, 13, 13, 10, 5, 15, 7, 24, 28, 9, 18, 5, 9, 5, 10, 7, 10, 5, 8, 9, 7, 8, 5, 27, 5, 5, 14, 11, 5, 21, 6, 11, 7, 6, 12, 8, 15, 10, 5, 6, 5, 21, 10, 12, 10, 13, 12, 8, 6, 6, 6, 41, 8, 5, 11, 5, 6, 15, 8, 11, 5, 5, 6, 5, 5, 6, 11, 6, 12, 5, 22, 29, 10, 26, 18, 50, 14, 12, 14, 19, 5, 9, 9, 5, 11, 16, 10, 16, 7, 7, 8, 9, 10, 11, 5, 5, 11, 9, 19, 5, 5, 7, 6, 5, 5, 6, 5, 12, 5, 5, 5, 15, 9, 5, 9, 7, 8, 6, 47, 5, 5, 5, 18, 12, 18, 11, 7, 32, 8, 7, 6, 36, 6, 7, 8, 10, 6, 5, 6, 5, 7, 6, 6, 5, 5, 5, 13, 23, 8, 5, 11, 9, 9, 7, 7, 6, 5, 6, 5, 7, 5, 7, 5, 5, 5, 14, 8, 6, 5, 9, 6, 10, 5, 5, 7, 7, 8, 5, 7, 6, 9, 6, 5, 5, 8, 7, 5, 5, 6, 10, 8, 8, 5, 9, 10, 7, 6, 7, 17, 9, 6, 10, 16, 5, 8, 5, 5, 6, 5, 14, 6, 11, 6, 8, 12, 6, 5, 8, 20, 5, 6, 11, 6, 6, 6, 5, 34, 8, 5, 6, 5, 19, 8, 10, 12, 7, 6, 14, 9, 19, 10, 6, 6, 8, 5, 5, 6, 7, 12, 8, 9, 6, 91, 89, 36, 12, 12, 58, 17, 55, 23, 20, 41, 83, 163, 21, 166, 14, 13, 101, 29, 61, 199, 78, 76, 118, 56, 129, 26, 35, 90, 11, 77, 6, 29, 57, 6, 65, 5, 5, 31, 15, 33, 15, 27, 20, 52, 17, 35, 54, 54, 17, 112, 6, 24, 17, 13, 5, 14, 21, 17, 6, 10, 12, 5, 19, 22, 31, 8, 6, 9, 42, 39, 7, 13, 81, 10, 6, 5, 6, 17, 7, 9, 12, 19, 16, 5, 8, 6, 17, 5, 8, 11, 10, 7, 5, 5, 17, 5, 6, 33, 5, 11, 5, 27, 14, 10, 5, 21, 5, 9, 12, 10, 35, 9, 14, 14, 6, 6, 6, 5, 7, 19, 12, 12, 7, 5, 7, 5, 9, 9, 14, 18, 18, 7, 13, 8, 10, 8, 9, 12, 6, 5, 6, 10, 9, 8, 10, 6, 13, 7, 11, 9, 7, 5, 5, 10, 7, 7, 22, 5, 5, 5, 6, 12, 18, 5, 5, 7, 5, 12, 10, 5, 6, 7, 8, 27, 5, 6, 36, 5, 8, 11, 25, 11, 5, 6, 6, 6, 5, 5, 8, 8, 8, 10, 6, 6, 13, 9, 5, 5, 7, 7, 5, 5, 5, 29, 5, 21, 9, 10, 8, 5, 8, 5, 7, 8, 8, 10, 6, 8, 5, 7, 8, 6, 6, 6, 6, 5, 5, 18, 5, 9, 12, 5, 5, 8, 5, 5, 5, 16, 5, 6, 7, 5, 24, 5, 5, 6, 15, 9, 8, 10, 16, 11, 5, 5, 18, 25, 7, 6, 8, 5, 5, 8, 6, 8, 8, 5, 7, 5, 22, 7, 8, 5, 7, 7, 10, 18, 8, 5, 5, 7, 5, 5, 7, 7, 10, 6, 5, 8, 9, 15, 6, 5, 8, 8, 5, 7, 5, 5, 8, 5, 15, 18, 9, 7, 7, 10, 5, 6, 5, 9, 5, 18, 6, 13, 5, 5, 11, 8, 5, 10, 7, 5, 5, 10, 5, 6, 6, 9, 6, 6, 6, 5, 5, 7, 6, 7, 7, 8, 5, 6, 8, 6, 5, 5, 5, 5, 5, 5, 5, 7, 6, 6, 6, 8, 11, 5, 5, 11, 6, 6, 6, 6, 28, 11, 5, 13, 9, 7, 5, 6, 5, 6, 5, 9, 7, 6, 9, 5, 18, 31, 19, 9, 5, 6, 10, 5, 5, 5, 329, 17, 31, 5, 9, 28, 5, 7, 10, 6, 26, 22, 9, 11, 6, 7, 10, 10, 8, 9, 25, 6, 5, 12, 5, 6, 7, 10, 6, 8, 5, 19, 7, 7, 36, 10, 5, 19, 6, 6, 11, 6, 6, 9, 11, 6, 5, 5, 8, 7, 5, 6, 24, 8, 5, 8, 7, 10, 7, 12, 14, 15, 5, 14, 13, 5, 12, 7, 10, 6, 12, 5, 7, 5, 10, 7, 5, 6, 15, 7, 5, 10, 162, 5, 7, 7, 10, 6, 7, 5, 8, 6, 11, 5, 6, 5, 7, 13, 8, 5, 5, 5, 8, 6, 7, 5, 11, 16, 9, 7, 5, 6, 5, 5, 15, 6, 7, 5, 7, 7, 31, 24, 10, 13, 5, 6, 6, 16, 9, 13, 7, 55, 17, 6, 7, 5, 16, 6, 7, 7, 9, 5, 6, 9, 5, 13, 7, 10, 6, 7, 6, 8, 8, 10, 7, 6, 5, 7, 7, 76, 25, 7, 30, 8, 11, 15, 6, 5, 6, 8, 9, 7, 6, 29, 5, 9, 5, 5, 7, 11, 19, 20, 5, 26, 37, 7, 6, 7, 17, 8, 5, 8, 5, 5, 6, 6, 5, 11, 17, 5, 7, 11, 17, 16, 5, 17, 8, 8, 6, 6, 5, 5, 5, 5, 5, 17, 24, 10, 6, 6, 6, 5, 16, 7, 6, 6, 22, 8, 56, 5, 5, 24, 30, 24, 13, 12, 19, 8, 5, 5, 5, 5, 5, 5, 6, 7, 5, 11, 6, 9, 6, 5, 6, 5, 5, 12, 5, 5, 9, 5, 6, 10, 14, 13, 20, 11, 7, 7, 12, 7, 5, 7, 8, 5, 5, 5, 5, 8, 7, 5, 8, 6, 6, 5, 5, 5, 5, 5, 5, 5, 15, 5, 13, 14, 21, 12, 6, 6, 7, 30, 11, 7, 5, 11, 8, 40, 5, 5, 12, 5, 7, 10, 8, 5, 5, 7, 5, 6, 5, 6, 7, 5, 5, 5, 5, 8, 5, 9, 8, 17, 17, 6, 6, 5, 5, 5, 6, 6, 6, 6, 5, 5, 6, 9, 6, 6, 5, 14, 7, 6, 10, 15, 5, 13, 8, 12, 5, 7, 5, 5, 6, 15, 6, 10, 7, 6, 6, 5, 5, 5, 5, 6, 6, 9, 13, 7, 8, 8, 5, 5, 10, 5, 12, 5, 5, 8, 5, 5, 5, 8, 5, 17, 5, 5, 14, 6, 10, 5, 5, 5, 7, 9, 7, 9, 5, 5, 6, 6, 5, 6, 12, 22, 5, 6])\n",
            "[  11.   56. 1945. ...   29.   10.   11.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.656046Z",
          "start_time": "2022-02-19T14:34:11.653325Z"
        },
        "id": "mQIVrOIR4WSn"
      },
      "outputs": [],
      "source": [
        "def get_neg_v_negative_sampling(batch_size:int, n_neg_sample:int):\n",
        "    \"\"\"\n",
        "    위에서 정의한 sample_table에서 (batch_size, n_neg_sample) shape만큼 랜덤 추출해 \"네거티브 샘플 메트릭스\"를 생성\n",
        "    np.random.choice() 함수 활용 (위에서 정의한 sample_table을 함수의 argument로 사용)\n",
        "    \"\"\"\n",
        "    \n",
        "    neg_v = np.random.choice(sample_table,(batch_size,n_neg_sample)).tolist()\n",
        "    \n",
        "    return neg_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:12.345976Z",
          "start_time": "2022-02-19T14:34:12.333448Z"
        },
        "id": "8wwT4Af04WSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00fd500a-1ce6-4ef7-a01e-38d5137f7128"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[36, 4419, 52, 2799, 2018],\n",
              " [5555, 307, 4185, 371, 5439],\n",
              " [3478, 4770, 1336, 271, 180],\n",
              " [57, 3457, 1687, 4851, 1633]]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "get_neg_v_negative_sampling(4, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLnDXPvJ4WSo"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5UubCzK4WSo"
      },
      "source": [
        "### 미니 튜토리얼\n",
        "- 아래 튜토리얼을 따라하며 Skip-Gram 모델의 `forward` 및 `loss` 연산 방식을 이해하자\n",
        "- Reference\n",
        "    - [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "    - [torch bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n",
        "    - [Skip-Gram negative sampling loss function 설명 영문 블로그](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#Derivation-of-Cost-Function-in-Negative-Sampling)\n",
        "    - [Skip-Gram negative sampling loss function 설명 한글 블로그](https://reniew.github.io/22/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:46.954048Z",
          "start_time": "2022-02-19T12:51:46.951529Z"
        },
        "id": "IAR68hsY4WSo"
      },
      "outputs": [],
      "source": [
        "# hyper parameter example\n",
        "emb_size = 30000 # vocab size\n",
        "emb_dimension = 300 # word embedding 차원\n",
        "n_neg_sample = 5\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.340056Z",
          "start_time": "2022-02-19T12:51:47.300999Z"
        },
        "id": "zzOsVUn94WSo"
      },
      "outputs": [],
      "source": [
        "# 1. Embedding Matrix와 Context Matrix를 생성\n",
        "u_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)\n",
        "v_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.352240Z",
          "start_time": "2022-02-19T12:51:49.341437Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7J_ADc44WSo",
        "outputId": "b4d60f8d-627a-40bf-f1db-33aadce825e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target word idx : tensor([24460, 10634,  2864, 23952,  3320, 15187, 19625, 26546, 27339,  3920,\n",
            "        25847,  6023,  5055,  7070,  6291, 10245, 15926,   641, 20178,  4565,\n",
            "         4784, 26715, 16955, 28742, 17947, 19774,  8065, 22605,  3061, 28965,\n",
            "         3056, 17963]) Pos context word idx : tensor([23224,  5636, 23712,  5234,  3991, 17897, 25123, 17938, 19634, 24228,\n",
            "          693,   799, 25457,  1308, 28935, 25696,  5601, 23878,  8312,  1292,\n",
            "        21380, 16974,  9318,  9578, 12915, 29271, 26465, 20572,  2362, 25929,\n",
            "        19754, 29080]) Neg context word idx : [[870, 3538, 898, 92, 628], [1013, 4847, 5514, 4234, 1501], [4707, 625, 1568, 759, 1982], [520, 1871, 3981, 1536, 362], [56, 3895, 2548, 14, 5258], [1031, 549, 684, 4225, 623], [2914, 1328, 1041, 2770, 2578], [702, 3038, 4881, 93, 4957], [3875, 285, 115, 2513, 331], [1091, 2825, 538, 114, 3724], [1486, 753, 1631, 4721, 5034], [871, 1909, 3140, 465, 1338], [3282, 346, 1813, 470, 87], [621, 4216, 2544, 4083, 52], [1644, 139, 5055, 545, 2120], [4605, 1084, 4001, 4429, 1002], [2885, 1088, 96, 1433, 887], [3571, 1969, 92, 1686, 331], [1434, 4165, 1697, 4881, 130], [2500, 623, 2162, 4171, 2999], [2393, 1562, 1223, 5626, 1514], [3660, 84, 4881, 1200, 86], [31, 3457, 1893, 222, 4959], [1912, 980, 5019, 5458, 801], [4454, 2341, 1297, 947, 5379], [1622, 3519, 3034, 5102, 1586], [3219, 139, 2009, 3204, 1854], [47, 940, 3693, 5578, 4241], [139, 4999, 285, 2658, 3016], [5248, 2568, 230, 1470, 2620], [23, 2624, 3530, 1040, 3140], [4389, 5407, 5183, 324, 128]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. wid(단어 인덱스)를 임의로 생성\n",
        "pos_u = torch.randint(high = emb_size, size = (batch_size,))\n",
        "pos_v = torch.randint(high = emb_size, size = (batch_size,))\n",
        "neg_v = get_neg_v_negative_sampling(batch_size, n_neg_sample)\n",
        "print(f\"Target word idx : {pos_u} Pos context word idx : {pos_v} Neg context word idx : {neg_v}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.364020Z",
          "start_time": "2022-02-19T12:51:49.353486Z"
        },
        "id": "4iEG0nCZ4WSo"
      },
      "outputs": [],
      "source": [
        "# 3. tensor로 변환\n",
        "pos_u = Variable(torch.LongTensor(pos_u)).to(device)\n",
        "pos_v = Variable(torch.LongTensor(pos_v)).to(device)\n",
        "neg_v = Variable(torch.LongTensor(neg_v)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:51.391896Z",
          "start_time": "2022-02-19T12:51:51.387084Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqbNbajG4WSo",
        "outputId": "db0ef7b2-d900-4950-a182-4c01a3cfa9e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of pos_u embedding : torch.Size([32, 300])\n",
            " shape of pos_v embedding : torch.Size([32, 300])\n",
            " shape of neg_v embedding : torch.Size([32, 5, 300])\n"
          ]
        }
      ],
      "source": [
        "# 4. wid로 각각의 embedding matrix에서 word embedding 값을 가져오기\n",
        "pos_u = u_embedding(pos_u)\n",
        "pos_v = v_embedding(pos_v)\n",
        "neg_v = v_embedding(neg_v)\n",
        "print(f\"shape of pos_u embedding : {pos_u.shape}\\n shape of pos_v embedding : {pos_v.shape}\\n shape of neg_v embedding : {neg_v.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.121477Z",
          "start_time": "2022-02-19T12:51:52.646148Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDWUrSwo4WSo",
        "outputId": "608c09bb-6865-442c-a00d-e6e8ba771aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of pos logits : torch.Size([32])\n",
            "\n",
            "shape of logits : torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "# 5. dot product \n",
        "pos_score = torch.mul(pos_u, pos_v) # 행렬 element-wise 곱\n",
        "pos_score = torch.sum(pos_score, dim=1)\n",
        "print(f\"shape of pos logits : {pos_score.shape}\\n\")\n",
        "\n",
        "neg_score = torch.bmm(neg_v, pos_u.unsqueeze(dim=2)).squeeze()\n",
        "print(f\"shape of logits : {neg_score.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.670418Z",
          "start_time": "2022-02-19T12:51:53.665671Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adOpcoL54WSo",
        "outputId": "df7bcd9f-797e-4f52-d951-b56316eebee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos logits : -241.4199676513672\n",
            "neg logits : -1373.56982421875\n",
            "Loss : 1614.98974609375\n"
          ]
        }
      ],
      "source": [
        "# 6. loss 구하기\n",
        "pos_score = F.logsigmoid(pos_score)\n",
        "neg_score = F.logsigmoid(-1*neg_score) # negative의 logit은 minimize 하기 위해 -1 곱함\n",
        "print(f\"pos logits : {pos_score.sum()}\")\n",
        "print(f\"neg logits : {neg_score.sum()}\")\n",
        "loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "print(f\"Loss : {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muEceOGZ4WSo"
      },
      "source": [
        "### Skip-gram 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `SkipGram` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `device` : 연산 장치 종류\n",
        "    - 생성자에서 생성해야할 변수 \n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `u_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (target_word)\n",
        "        - `v_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (context_word)\n",
        "    - 메소드\n",
        "        - `init_embedding()` (제공됨)\n",
        "            - 엠베딩 메트릭스 값을 초기화\n",
        "        - `forward()`\n",
        "            - 위 튜토리얼과 같이 dot product를 수행한 후 score를 생성\n",
        "            - loss를 반환 (loss 설명 추가)\n",
        "        - `save_emedding()` (제공됨)\n",
        "            - `u_embedding`의 단어 엠베딩 값을 단어 별로 파일에 저장\n",
        "    - 주의 사항     \n",
        "        - `nn.Module`를 부모 클래스로 상속 받음 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:15.731306Z",
          "start_time": "2022-02-19T14:34:15.721129Z"
        },
        "id": "pnmMamP44WSo"
      },
      "outputs": [],
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size:int, emb_dimension:int, device:str):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embedding = nn.Embedding(vocab_size, emb_dimension, sparse=True).to(device)\n",
        "        self.v_embedding = nn.Embedding(vocab_size, emb_dimension, sparse=True).to(device)\n",
        "        self.init_embedding()\n",
        "    \n",
        "    \n",
        "    def init_embedding(self):\n",
        "        \"\"\"\n",
        "        u_embedding과 v_embedding 메트릭스 값을 초기화\n",
        "        \"\"\"\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        self.u_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embedding.weight.data.uniform_(-0, 0)\n",
        "    \n",
        "    \n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \"\"\"\n",
        "        dot product를 수행한 후 score를 생성\n",
        "        loss 반환\n",
        "        \"\"\"    \n",
        "            \n",
        "        # 각각의 embedding matrix에서 word embedding 값을 가져오기\n",
        "        pos_u = self.u_embedding(pos_u)\n",
        "        pos_v = self.v_embedding(pos_v)\n",
        "        neg_v = self.v_embedding(neg_v)\n",
        "\n",
        "        # dot product \n",
        "        pos_score = torch.sum(torch.mul(pos_u, pos_v), dim=1)\n",
        "        neg_score = torch.bmm(neg_v, pos_u.unsqueeze(dim=2)).squeeze()\n",
        "        \n",
        "        # loss 구하기\n",
        "        pos_score = F.logsigmoid(pos_score)\n",
        "        neg_score = F.logsigmoid(-1*neg_score) # negative의 logit은 minimize 하기 위해 -1 곱함\n",
        "        loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def save_embedding(self, id2word, file_name, use_cuda):\n",
        "        \"\"\"\n",
        "        'file_name' 위치에 word와 word_embedding을 line-by로 저장\n",
        "        파일의 첫 줄은 '단어 개수' 그리고 '단어 embedding 사이즈' 값을 입력해야 함\n",
        "        \"\"\"\n",
        "        if use_cuda: # parameter를 gpu 메모리에서 cpu 메모리로 옮김\n",
        "            embedding = self.u_embedding.weight.cpu().data.numpy()\n",
        "        else:\n",
        "            embedding = self.u_embedding.weight.data.numpy()\n",
        "\n",
        "        with open(file_name, 'w') as writer:\n",
        "            # 파일의 첫 줄은 '단어 개수' 그리고 '단어 embedding 사이즈' 값을 입력해야 함\n",
        "            writer.write(f\"{len(id2word)} {embedding.shape[-1]}\\n\")\n",
        "            \n",
        "            for wid, word in id2word.items():\n",
        "                e = embedding[wid]\n",
        "                e = \" \".join([str(e_) for e_ in e])\n",
        "                writer.write(f\"{word} {e}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqqMo0zL4WSo"
      },
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSWd5gV24WSp"
      },
      "source": [
        "### Skip-Gram 방식의  Word2Vec 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `Word2Vec` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()`) 입력 매개 변수\n",
        "        - `input_file` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `device` : 연상 장치 종류\n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `min_count` : 사전에 추가될 단어의 최소 등장 빈도\n",
        "    - 생성자에서 생성해야 할 변수 \n",
        "        - `docs` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `word2count`, `word2id`, `id2word` : 위에서 구현한 `make_vocab()` 함수의 반환 값\n",
        "        - `device` : 연산 장치 종류\n",
        "        - `emb_size` : vocab의 (unique한) 단어 종류 \n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `model` : `SkipGram` 클래스의 인스턴스\n",
        "        - `optimizer` : `SGD` 클래스의 인스턴스\n",
        "    - 메소드\n",
        "        - `train()`\n",
        "            - 입력 매개변수 \n",
        "                - `train_dataloader`\n",
        "            - Iteration 횟수만큼 input_file 학습 데이터를 학습한다. 매 epoch마다 for loop 돌면서 batch 단위 학습 데이터를 skip gram 모델에 학습함. 학습이 끝나면 word embedding을 output_file_name 파일에 저장.\n",
        "- Reference\n",
        "    - [Optimizer - SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:20.503555Z",
          "start_time": "2022-02-19T14:34:20.491585Z"
        },
        "id": "Td-GQrqI4WSp"
      },
      "outputs": [],
      "source": [
        "from bs4.builder import TreeBuilderRegistry\n",
        "class Word2Vec:\n",
        "    def __init__(self, \n",
        "                input_file: List[str],\n",
        "                output_file_name: str,\n",
        "                 device: str,\n",
        "                 emb_dimension=300,\n",
        "                 batch_size = 64,\n",
        "                 window_size=5,\n",
        "                 n_neg_sample = 5,\n",
        "                 iteration=1,\n",
        "                 lr = 0.02,\n",
        "                 min_count=5):\n",
        "        self.docs = input_file\n",
        "        self.output_file_name = output_file_name\n",
        "        self.word2count, self.word2id, self.id2word = make_vocab(input_file,min_count)\n",
        "        self.device = device\n",
        "        self.emb_size = len(self.word2id)\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.batch_size = batch_size\n",
        "        self.window_size = window_size\n",
        "        self.n_neg_sample = n_neg_sample\n",
        "        self.iteration = iteration\n",
        "        self.lr = lr\n",
        "        self.model = SkipGram(vocab_size = len(self.word2id), emb_dimension = emb_dimension, device = device)\n",
        "        self.optimizer = torch.optim.SGD(self.model.parameters(),lr=lr) # torch.optim.SGD 클래스 사용\n",
        "\n",
        "        # train() 함수에서 만든 임베딩 결과 파일들을 저장할 폴더 생성 (os.makedirs 사용)\n",
        "        os.makedirs('./'+output_file_name,exist_ok=True)\n",
        "        \n",
        "    \n",
        "    def train(self, train_dataloader):\n",
        "        \n",
        "        # lr 값을 조절하는 스케줄러 인스턴스 변수를 생성\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer = self.optimizer,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps=self.iteration*len(train_dataloader)\n",
        "        )\n",
        "        \n",
        "        for epoch in range(self.iteration):\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "            print(f\"*****Epoch {epoch} Total Step {len(train_dataloader)}*****\")\n",
        "            total_loss, batch_loss, batch_step = 0,0,0\n",
        "\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                batch_step+=1\n",
        "                pos_u, pos_v = batch\n",
        "                # negative data 생성\n",
        "                neg_v = get_neg_v_negative_sampling(pos_u.shape[0], self.n_neg_sample)\n",
        "                \n",
        "                # 데이터를 tensor화 & device 설정\n",
        "                pos_u = Variable(torch.LongTensor(pos_u)).to(device)\n",
        "                pos_v = Variable(torch.LongTensor(pos_v)).to(device)\n",
        "                neg_v = Variable(torch.LongTensor(neg_v)).to(device)\n",
        "\n",
        "                # model의 gradient 초기화\n",
        "                self.model.init_embedding()\n",
        "                # optimizer의 gradient 초기화\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                loss = self.model.forward(pos_u,pos_v,neg_v)\n",
        "\n",
        "                # loss 계산\n",
        "                loss.backward()\n",
        "                # optimizer 업데이트\n",
        "                self.optimizer.step()\n",
        "                # scheduler 업데이트\n",
        "                self.scheduler.step()\n",
        "\n",
        "                batch_loss += loss.item()\n",
        "                total_loss += loss.item()\n",
        "                \n",
        "                if (step%500 == 0) and (step!=0):\n",
        "                    print(f\"Step: {step} Loss: {batch_loss/batch_step:.4f} lr: {self.optimizer.param_groups[0]['lr']:.4f}\")\n",
        "                    # 변수 초기화    \n",
        "                    batch_loss, batch_step = 0,0\n",
        "            \n",
        "            print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "            print(f\"*****Epoch {epoch} Train Finished*****\\n\")\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Saving Embedding...*****\")\n",
        "            self.model.save_embedding(self.id2word, os.path.join(self.output_file_name, f'w2v_{epoch}.txt'), True if 'cuda' in self.device.type else False)\n",
        "            print(f\"*****Epoch {epoch} Embedding Saved at {os.path.join(self.output_file_name, f'w2v_{epoch}.txt')}*****\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:29.561892Z",
          "start_time": "2022-02-19T14:34:26.103659Z"
        },
        "id": "Ywx9R8n24WSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214710cd-2c0e-4742-bd84-2ae996fe95ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 6916.32it/s]\n"
          ]
        }
      ],
      "source": [
        "output_file = os.path.join(\".\", \"word2vec_wiki\")\n",
        "# Word2Vec 클래스의 인스턴스 생성\n",
        "w2v = Word2Vec(docs, output_file, device, n_neg_sample=10, iteration=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:34.615469Z",
          "start_time": "2022-02-19T14:34:34.055502Z"
        },
        "id": "ufBxjKxN4WSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bbae6cb-b608-4e08-b601-af7918be619a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16013"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "# 학습 데이터 셋 및 데이터 로더 생성 (위에서 생성한 w2v의 attribute들을 argument에 적절히 넣기)\n",
        "dataset = CustomDataset(docs, w2v.word2id, w2v.window_size)\n",
        "train_dataloader = DataLoader(dataset,w2v.batch_size,shuffle=True)\n",
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:45:38.362817Z",
          "start_time": "2022-02-19T14:34:37.382371Z"
        },
        "id": "9JBUrUJ34WSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5791a6d3-508c-498f-f7f8-6bec941630c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Epoch 0 Train Start*****\n",
            "*****Epoch 0 Total Step 16013*****\n",
            "Step: 500 Loss: 487.9756 lr: 0.0198\n",
            "Step: 1000 Loss: 487.9756 lr: 0.0196\n",
            "Step: 1500 Loss: 487.9756 lr: 0.0194\n",
            "Step: 2000 Loss: 487.9756 lr: 0.0192\n",
            "Step: 2500 Loss: 487.9756 lr: 0.0190\n",
            "Step: 3000 Loss: 487.9756 lr: 0.0188\n",
            "Step: 3500 Loss: 487.9756 lr: 0.0185\n",
            "Step: 4000 Loss: 487.9756 lr: 0.0183\n",
            "Step: 4500 Loss: 487.9756 lr: 0.0181\n",
            "Step: 5000 Loss: 487.9756 lr: 0.0179\n",
            "Step: 5500 Loss: 487.9756 lr: 0.0177\n",
            "Step: 6000 Loss: 487.9756 lr: 0.0175\n",
            "Step: 6500 Loss: 487.9756 lr: 0.0173\n",
            "Step: 7000 Loss: 487.9756 lr: 0.0171\n",
            "Step: 7500 Loss: 487.9756 lr: 0.0169\n",
            "Step: 8000 Loss: 487.9756 lr: 0.0167\n",
            "Step: 8500 Loss: 487.9756 lr: 0.0165\n",
            "Step: 9000 Loss: 487.9756 lr: 0.0163\n",
            "Step: 9500 Loss: 487.9756 lr: 0.0160\n",
            "Step: 10000 Loss: 487.9756 lr: 0.0158\n",
            "Step: 10500 Loss: 487.9756 lr: 0.0156\n",
            "Step: 11000 Loss: 487.9756 lr: 0.0154\n",
            "Step: 11500 Loss: 487.9756 lr: 0.0152\n",
            "Step: 12000 Loss: 487.9756 lr: 0.0150\n",
            "Step: 12500 Loss: 487.9756 lr: 0.0148\n",
            "Step: 13000 Loss: 487.9756 lr: 0.0146\n",
            "Step: 13500 Loss: 487.9756 lr: 0.0144\n",
            "Step: 14000 Loss: 487.9756 lr: 0.0142\n",
            "Step: 14500 Loss: 487.9756 lr: 0.0140\n",
            "Step: 15000 Loss: 487.9756 lr: 0.0138\n",
            "Step: 15500 Loss: 487.9756 lr: 0.0135\n",
            "Step: 16000 Loss: 487.9756 lr: 0.0133\n",
            "Epoch 0 Total Mean Loss : 487.9509\n",
            "*****Epoch 0 Train Finished*****\n",
            "\n",
            "*****Epoch 0 Saving Embedding...*****\n",
            "*****Epoch 0 Embedding Saved at ./word2vec_wiki/w2v_0.txt*****\n",
            "\n",
            "*****Epoch 1 Train Start*****\n",
            "*****Epoch 1 Total Step 16013*****\n",
            "Step: 500 Loss: 487.9756 lr: 0.0131\n",
            "Step: 1000 Loss: 487.9756 lr: 0.0129\n",
            "Step: 1500 Loss: 487.9756 lr: 0.0127\n",
            "Step: 2000 Loss: 487.9756 lr: 0.0125\n",
            "Step: 2500 Loss: 487.9756 lr: 0.0123\n",
            "Step: 3000 Loss: 487.9756 lr: 0.0121\n",
            "Step: 3500 Loss: 487.9756 lr: 0.0119\n",
            "Step: 4000 Loss: 487.9756 lr: 0.0117\n",
            "Step: 4500 Loss: 487.9756 lr: 0.0115\n",
            "Step: 5000 Loss: 487.9756 lr: 0.0113\n",
            "Step: 5500 Loss: 487.9756 lr: 0.0110\n",
            "Step: 6000 Loss: 487.9756 lr: 0.0108\n",
            "Step: 6500 Loss: 487.9756 lr: 0.0106\n",
            "Step: 7000 Loss: 487.9756 lr: 0.0104\n",
            "Step: 7500 Loss: 487.9756 lr: 0.0102\n",
            "Step: 8000 Loss: 487.9756 lr: 0.0100\n",
            "Step: 8500 Loss: 487.9756 lr: 0.0098\n",
            "Step: 9000 Loss: 487.9756 lr: 0.0096\n",
            "Step: 9500 Loss: 487.9756 lr: 0.0094\n",
            "Step: 10000 Loss: 487.9756 lr: 0.0092\n",
            "Step: 10500 Loss: 487.9756 lr: 0.0090\n",
            "Step: 11000 Loss: 487.9756 lr: 0.0088\n",
            "Step: 11500 Loss: 487.9756 lr: 0.0085\n",
            "Step: 12000 Loss: 487.9756 lr: 0.0083\n",
            "Step: 12500 Loss: 487.9756 lr: 0.0081\n",
            "Step: 13000 Loss: 487.9756 lr: 0.0079\n",
            "Step: 13500 Loss: 487.9756 lr: 0.0077\n",
            "Step: 14000 Loss: 487.9756 lr: 0.0075\n",
            "Step: 14500 Loss: 487.9756 lr: 0.0073\n",
            "Step: 15000 Loss: 487.9756 lr: 0.0071\n",
            "Step: 15500 Loss: 487.9756 lr: 0.0069\n",
            "Step: 16000 Loss: 487.9756 lr: 0.0067\n",
            "Epoch 1 Total Mean Loss : 487.9509\n",
            "*****Epoch 1 Train Finished*****\n",
            "\n",
            "*****Epoch 1 Saving Embedding...*****\n",
            "*****Epoch 1 Embedding Saved at ./word2vec_wiki/w2v_1.txt*****\n",
            "\n",
            "*****Epoch 2 Train Start*****\n",
            "*****Epoch 2 Total Step 16013*****\n",
            "Step: 500 Loss: 487.9756 lr: 0.0065\n",
            "Step: 1000 Loss: 487.9756 lr: 0.0062\n",
            "Step: 1500 Loss: 487.9756 lr: 0.0060\n",
            "Step: 2000 Loss: 487.9756 lr: 0.0058\n",
            "Step: 2500 Loss: 487.9756 lr: 0.0056\n",
            "Step: 3000 Loss: 487.9756 lr: 0.0054\n",
            "Step: 3500 Loss: 487.9756 lr: 0.0052\n",
            "Step: 4000 Loss: 487.9756 lr: 0.0050\n",
            "Step: 4500 Loss: 487.9756 lr: 0.0048\n",
            "Step: 5000 Loss: 487.9756 lr: 0.0046\n",
            "Step: 5500 Loss: 487.9756 lr: 0.0044\n",
            "Step: 6000 Loss: 487.9756 lr: 0.0042\n",
            "Step: 6500 Loss: 487.9756 lr: 0.0040\n",
            "Step: 7000 Loss: 487.9756 lr: 0.0038\n",
            "Step: 7500 Loss: 487.9756 lr: 0.0035\n",
            "Step: 8000 Loss: 487.9756 lr: 0.0033\n",
            "Step: 8500 Loss: 487.9756 lr: 0.0031\n",
            "Step: 9000 Loss: 487.9756 lr: 0.0029\n",
            "Step: 9500 Loss: 487.9756 lr: 0.0027\n",
            "Step: 10000 Loss: 487.9756 lr: 0.0025\n",
            "Step: 10500 Loss: 487.9756 lr: 0.0023\n",
            "Step: 11000 Loss: 487.9756 lr: 0.0021\n",
            "Step: 11500 Loss: 487.9756 lr: 0.0019\n",
            "Step: 12000 Loss: 487.9756 lr: 0.0017\n",
            "Step: 12500 Loss: 487.9756 lr: 0.0015\n",
            "Step: 13000 Loss: 487.9756 lr: 0.0013\n",
            "Step: 13500 Loss: 487.9756 lr: 0.0010\n",
            "Step: 14000 Loss: 487.9756 lr: 0.0008\n",
            "Step: 14500 Loss: 487.9756 lr: 0.0006\n",
            "Step: 15000 Loss: 487.9756 lr: 0.0004\n",
            "Step: 15500 Loss: 487.9756 lr: 0.0002\n",
            "Step: 16000 Loss: 487.9756 lr: 0.0000\n",
            "Epoch 2 Total Mean Loss : 487.9509\n",
            "*****Epoch 2 Train Finished*****\n",
            "\n",
            "*****Epoch 2 Saving Embedding...*****\n",
            "*****Epoch 2 Embedding Saved at ./word2vec_wiki/w2v_2.txt*****\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 학습\n",
        "w2v.train(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(w2v.model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-b21v0BYrsL",
        "outputId": "b9babaf5-1712-443f-c0c4-34b4a6e28738"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 1.4579e-03,  1.1121e-04, -1.1798e-03,  ..., -4.9304e-04,\n",
              "         -1.1774e-03,  1.6205e-03],\n",
              "        [ 6.6293e-04,  6.0215e-04,  9.7566e-04,  ..., -2.8999e-04,\n",
              "          1.2292e-03,  1.2270e-03],\n",
              "        [-1.4698e-03,  6.5783e-04, -1.8426e-04,  ...,  9.6384e-04,\n",
              "          7.0798e-04, -2.2876e-05],\n",
              "        ...,\n",
              "        [-1.4436e-03, -5.6932e-05,  1.3793e-03,  ..., -1.3117e-03,\n",
              "         -1.0156e-03,  1.5555e-03],\n",
              "        [ 1.3746e-03, -1.2367e-03, -1.7306e-04,  ...,  4.5078e-04,\n",
              "         -6.9273e-04,  7.2650e-04],\n",
              "        [-1.1881e-03,  1.6646e-04, -2.1749e-04,  ..., -1.2870e-03,\n",
              "          1.5915e-03, -1.1029e-03]], device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uTIm4vJ4WSp"
      },
      "source": [
        "### 유사한 단어 확인\n",
        "- 사전에 존재하는 단어들과 유사한 단어를 검색해보자. Gensim 패키지는 유사 단어 외에도 단어간의 유사도를 계산하는 여러 함수를 제공한다. 실험을 통해 word2vec의 한계점을 발견했다면 아래에 markdown으로 작성해보자. \n",
        "- [Gensim 패키지 document](https://radimrehurek.com/gensim/models/keyedvectors.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:47:59.605389Z",
          "start_time": "2022-02-19T14:47:59.368925Z"
        },
        "id": "AKpBuVlP4WSp"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:49:06.590460Z",
          "start_time": "2022-02-19T14:49:05.174241Z"
        },
        "id": "AWTCodimsAq8"
      },
      "outputs": [],
      "source": [
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/NLP/word2vec_wiki/w2v_2.txt', binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:49:11.324372Z",
          "start_time": "2022-02-19T14:49:11.315429Z"
        },
        "id": "MLMh_evrsAq9"
      },
      "outputs": [],
      "source": [
        "word_vectors.most_similar(positive=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vec의 한계점은?"
      ],
      "metadata": {
        "id": "X8lc8NQe4cT2"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "유형진 - Week3_1_assignment_solved.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}